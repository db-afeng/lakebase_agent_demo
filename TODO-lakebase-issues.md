# Lakebase Issues - Follow-up Required

This document tracks issues discovered during the Lakebase integration that require follow-up with the Databricks team.

---

## Issue 1: Branch TTL/Expiry Policy Not Working

**Status:** ðŸ”´ Blocked - Waiting on Databricks fix

**Description:**  
The `databricks postgres create-branch` command does not correctly accept the `ttl` or `expire_time` parameters, even when nested under `spec`.

**Expected Behavior:**  
Child branches should be created with a 6-hour TTL to auto-expire:
```bash
databricks postgres create-branch "projects/{project_id}" "{branch_name}" \
  --json '{"spec": {"source_branch_id": "{source_id}", "ttl": {"seconds": 21600}}}'
```

**Actual Behavior:**  
The CLI returns errors like:
- `Warning: unknown field: ttl`
- `Error: Expiration must be specified when creating a branch`

**Current Workaround:**  
Using `"no_expiry": true` instead, which means branches don't auto-expire:
```bash
--json '{"spec": {"source_branch_id": "{source_id}", "no_expiry": true}}'
```

**Impact:**  
- Development branches accumulate and need manual cleanup
- No automatic resource management for agent/worktree branches

**When Fixed:**  
Update `scripts/lakebase-branch.sh` line ~165 to use:
```bash
--json "{\"spec\": {\"source_branch_id\": \"${PRODUCTION_BRANCH_ID}\", \"ttl\": {\"seconds\": ${TTL_SECONDS}}}}"
```

---

## Issue 2: OAuth Credentials Have Read-Only Scopes (No DDL Support)

**Status:** ðŸ”´ Blocked - Waiting on Databricks fix/guidance

**Description:**  
The OAuth token generated by `databricks postgres generate-database-credential` only includes read-only IAM scopes, preventing DDL operations like CREATE TABLE.

**Token Scope (from JWT payload):**
```json
{
  "scope": "iam.current-user:read iam.groups:read iam.service-principals:read iam.users:read"
}
```

**Expected Behavior:**  
Tokens should include scopes that allow database DDL operations (CREATE, ALTER, DROP tables) when the user has those permissions granted in the database.

**Actual Behavior:**  
Even though the user has `CREATE` permission on the `public` schema (verified via `has_schema_privilege()`), Alembic migrations fail with:
```
psycopg.errors.InsufficientPrivilege: permission denied for schema public
```

**Current Workaround:**  
None - migrations cannot run with OAuth credentials.

**Questions for Databricks Team:**
1. Is there a way to generate credentials with DDL-capable scopes?
2. Should we use a different authentication method for migrations (e.g., service principal)?
3. Is there a project-level setting to enable write/admin access for OAuth tokens?

**When Fixed:**  
1. Test that `uv run alembic upgrade head` works after regenerating credentials
2. Verify the app can create tables via the API if needed
3. Update documentation in `.cursor/rules/project.mdc` if auth method changes

---

## Quick Test Commands

After Databricks fixes are applied, run these to verify:

```bash
# Regenerate credentials and run migrations
./scripts/lakebase-branch.sh

# Or just run migrations directly
uv run alembic upgrade head

# Test the app
uv run apx dev start
curl http://127.0.0.1:9001/api/products
```

---

**Last Updated:** 2026-02-02  
**Contact:** Follow up with Databricks Lakebase team
