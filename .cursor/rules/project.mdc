---
alwaysApply: true
---

# Cursor Agent Rules

# Do's and Don'ts
- Run `bun install` (or `uv run apx bun install`) to install dependencies and enable git hooks for automatic Lakebase branch setup.
- OpenAPI client auto-regenerates on code changes when dev servers are running - don't manually regenerate.
- Prefer running apx related commands via MCP server if it's available.
- Use the apx MCP `search_registry_components` and `add_component` tools to find and add shadcn/ui components.
- When using the API calls on the frontend, use error boundaries to handle errors.
- Run `apx dev check` command (via CLI or MCP) to check for errors in the project code after making changes.
- Avoid unnecessary restarts of the development servers
- **Databricks SDK:** Use the apx MCP `docs` tool to search Databricks SDK documentation instead of guessing or hallucinating API signatures.

## ‚ö†Ô∏è Post-Change Verification (REQUIRED)

**After making ANY changes, you MUST verify them:**

### Database Changes (models, migrations, schema)
After modifying `db_models.py`, creating migrations, or any database-related changes:
```bash
uv run alembic upgrade head
```
This applies migrations to your Lakebase branch. Always verify the command succeeds before continuing.

### Frontend Changes (UI, components, routes)
After making ANY frontend changes, **ALWAYS prefer the browser tool to verify**:
1. Use the native browser tool (cursor-browser-extension or cursor-ide-browser MCP) to take a screenshot
2. Navigate to the affected page/component and visually confirm the changes work as expected
3. If the browser tool is not available or not working, use Playwright MCP to automate browser actions (screenshots, clicks, etc.)

**Do NOT consider frontend work complete until you have visually verified it in the browser!**

## Package Management
- **Frontend:** Bun might not be present on user's $PATH. It's recommended to use prebundled bun (e.g., `uv run apx bun install` or `uv run apx bun add <dependency>`), unless user explicitly stated otherwise.
- **Python:** Always use `uv` (never `pip`)

## Component Management
- **Finding components:** Use MCP `search_registry_components` to search for available shadcn/ui components
- **Adding components:** Use MCP `add_component` or CLI `uv run apx components add <component> --yes` to add components
- **Component location:** If component was added to a wrong location (e.g. stored into `src/components` instead of `src/lakebase-agent-demo/ui/components`), move it to the proper folder
- **Component organization:** Prefer grouping components by functionality rather than by file type (e.g. `src/lakebase-agent-demo/ui/components/chat/`)

## Project Structure
Full-stack app: `src/lakebase-agent-demo/ui/` (React + Vite) and `src/lakebase-agent-demo/backend/` (FastAPI). Backend serves frontend at `/` and API at `/api`. API client auto-generated from OpenAPI schema.


## Models & API
- **3-model pattern:** `Entity` (DB), `EntityIn` (input), `EntityOut` (output)
- **API routes must have:** `response_model` and `operation_id` for client generation

## Frontend Rules
- **Routing:** `@tanstack/react-router` (routes in `src/lakebase-agent-demo/ui/routes/`)
- **Data fetching:** Always use `useXSuspense` hooks with `Suspense` and `Skeleton` components
- **Pattern:** Render static elements immediately, fetch API data with suspense
- **Components:** Use shadcn/ui, add to `src/lakebase-agent-demo/ui/components/`
- **Data access:** Use `selector()` function for clean destructuring (e.g., `const {data: profile} = useProfileSuspense(selector())`)

## Development Commands

**Start dev servers** (backend, frontend, OpenAPI watcher):
```bash
uv run apx dev start
```

**Check status** (shows running servers and ports):
```bash
uv run apx dev status
```

**Check for errors** (TypeScript, Python linting):
```bash
uv run apx dev check
```

**View logs:**
```bash
uv run apx dev logs              # Recent logs (default: last 10m)
uv run apx dev logs -d 1h        # Logs from last hour
uv run apx dev logs -f           # Follow/stream logs live
```

**Stop servers:**
```bash
uv run apx dev stop
```

**Build for production:**
```bash
uv run apx build
```

**Note:** OpenAPI client is automatically regenerated on every code change when dev servers are running. No manual regeneration needed.

## MCP Tools Reference

When the apx MCP server is available, use these tools:

| Tool | Description |
|------|-------------|
| `start` | Start development server and return the URL |
| `stop` | Stop the development server |
| `restart` | Restart the development server (preserves port if possible) |
| `logs` | Fetch recent dev server logs |
| `check` | Check project code for errors (runs tsc and ty checks in parallel) |
| `refresh_openapi` | Regenerate OpenAPI schema and API client |
| `search_registry_components` | Search shadcn registry components using semantic search |
| `add_component` | Add a component to the project |
| `docs` | Search Databricks SDK documentation for code examples and API references |
| `databricks_apps_logs` | Fetch logs from deployed Databricks app using Databricks CLI |
| `get_route_info` | Get code example for using a specific API route |

## Lakebase Database Development

This project uses **Databricks Lakebase** for PostgreSQL database. Each developer/agent gets their own isolated branch.

### ‚ö†Ô∏è CRITICAL: Database Setup Order

**ALWAYS run `lakebase-branch.sh` FIRST before ANY database operations:**

```bash
./scripts/lakebase-branch.sh
```

**üö´ NEVER run `alembic` commands directly before running `lakebase-branch.sh`!**

Why this matters:
- The script creates YOUR isolated Lakebase branch
- Running migrations before the script would apply changes to the WRONG database (parent branch)
- Each worktree/agent gets its own database branch - migrations must target that specific branch

### Automatic Setup via Git Hook

The project includes a **post-checkout hook** that automatically runs `lakebase-branch.sh` when you:
- Check out a new branch
- Create a new worktree
- Clone the repository

**Hooks are automatically enabled** when you run `bun install` (via the `prepare` script in package.json).

Once enabled, the Lakebase branch setup runs automatically - you don't need to run the script manually.

### Manual Setup (if hook not enabled)

1. **First:** Run `./scripts/lakebase-branch.sh` (creates branch, endpoint, credentials, runs migrations)
2. **Then:** Start development with `uv run apx dev start`
3. **Later:** If you add new migrations, run `uv run alembic upgrade head` (now targets YOUR branch)

The script handles everything including running migrations on your isolated branch.

### Branch Naming Convention

| Context | Lakebase Branch |
|---------|-----------------|
| Production (deployed app) | `production` (default branch) |
| Local dev on `main` | `{username}-main` (e.g., `alex-feng-main`) |
| Local dev on `feature-x` | `{username}-feature-x` |
| Agent worktree | `{username}-{parent-branch}-worktree-{worktree-dir}` |

Branch names are sanitized: slashes become hyphens, dots become hyphens, all lowercase.

### Databricks Postgres CLI Commands

**Requires Databricks CLI >= 0.287.0**

```bash
# List branches in project
databricks postgres list-branches "projects/{project_id}"

# Create a new branch from source (with 6-hour TTL)
databricks postgres create-branch "projects/{project_id}" "{branch_name}" \
  --json '{"spec": {"source_branch_id": "{source_branch_id}", "ttl": {"seconds": 21600}}}'

# Create a branch with no expiry
databricks postgres create-branch "projects/{project_id}" "{branch_name}" \
  --json '{"spec": {"source_branch_id": "{source_branch_id}", "no_expiry": true}}'

# Create endpoint on branch (with autoscaling)
databricks postgres create-endpoint "projects/{project_id}/branches/{branch_id}" "dev" \
  --json '{"spec": {"endpoint_type": "ENDPOINT_TYPE_READ_WRITE", "autoscaling_limit_min_cu": 0.5, "autoscaling_limit_max_cu": 2.0}}'

# Generate database credentials (returns OAuth token)
databricks postgres generate-database-credential \
  "projects/{project_id}/branches/{branch_id}/endpoints/{endpoint_id}"
# Note: Use your email as username and the returned 'token' as password
```

### Database Migrations (Alembic)

**‚ö†Ô∏è Only run these commands AFTER `./scripts/lakebase-branch.sh` has been executed!**

```bash
# Apply all migrations (only after lakebase-branch.sh!)
uv run alembic upgrade head

# Create new migration after model changes
uv run alembic revision --autogenerate -m "description"

# Rollback last migration
uv run alembic downgrade -1

# Show current migration status
uv run alembic current
```

### Environment Configuration

- **Local dev:** `.env` file (auto-generated by `lakebase-branch.sh`)
- **Production:** Configured in `databricks.yml` env vars
- **NEVER** commit credentials to git (`.env` is gitignored)
