---
alwaysApply: true
---

# Cursor Agent Rules

# Do's and Don'ts
- OpenAPI client auto-regenerates on code changes when dev servers are running - don't manually regenerate.
- Prefer running apx related commands via MCP server if it's available.
- Use the apx MCP `search_registry_components` and `add_component` tools to find and add shadcn/ui components.
- When using the API calls on the frontend, use error boundaries to handle errors.
- Run `apx dev check` command (via CLI or MCP) to check for errors in the project code after making changes.
- If agent has access to native browser tool, use it to verify changes on the frontend. If such tool is not present or is not working, use playwright MCP to automate browser actions (e.g. screenshots, clicks, etc.).
- Avoid unnecessary restarts of the development servers
- **Databricks SDK:** Use the apx MCP `docs` tool to search Databricks SDK documentation instead of guessing or hallucinating API signatures.

## Package Management
- **Frontend:** Bun might not be present on user's $PATH. It's recommended to use prebundled bun (e.g., `uv run apx bun install` or `uv run apx bun add <dependency>`), unless user explicitly stated otherwise.
- **Python:** Always use `uv` (never `pip`)

## Component Management
- **Finding components:** Use MCP `search_registry_components` to search for available shadcn/ui components
- **Adding components:** Use MCP `add_component` or CLI `uv run apx components add <component> --yes` to add components
- **Component location:** If component was added to a wrong location (e.g. stored into `src/components` instead of `src/lakebase-agent-demo/ui/components`), move it to the proper folder
- **Component organization:** Prefer grouping components by functionality rather than by file type (e.g. `src/lakebase-agent-demo/ui/components/chat/`)

## Project Structure
Full-stack app: `src/lakebase-agent-demo/ui/` (React + Vite) and `src/lakebase-agent-demo/backend/` (FastAPI). Backend serves frontend at `/` and API at `/api`. API client auto-generated from OpenAPI schema.


## Models & API
- **3-model pattern:** `Entity` (DB), `EntityIn` (input), `EntityOut` (output)
- **API routes must have:** `response_model` and `operation_id` for client generation

## Frontend Rules
- **Routing:** `@tanstack/react-router` (routes in `src/lakebase-agent-demo/ui/routes/`)
- **Data fetching:** Always use `useXSuspense` hooks with `Suspense` and `Skeleton` components
- **Pattern:** Render static elements immediately, fetch API data with suspense
- **Components:** Use shadcn/ui, add to `src/lakebase-agent-demo/ui/components/`
- **Data access:** Use `selector()` function for clean destructuring (e.g., `const {data: profile} = useProfileSuspense(selector())`)

## Development Commands

**Start dev servers** (backend, frontend, OpenAPI watcher):
```bash
uv run apx dev start
```

**Check status** (shows running servers and ports):
```bash
uv run apx dev status
```

**Check for errors** (TypeScript, Python linting):
```bash
uv run apx dev check
```

**View logs:**
```bash
uv run apx dev logs              # Recent logs (default: last 10m)
uv run apx dev logs -d 1h        # Logs from last hour
uv run apx dev logs -f           # Follow/stream logs live
```

**Stop servers:**
```bash
uv run apx dev stop
```

**Build for production:**
```bash
uv run apx build
```

**Note:** OpenAPI client is automatically regenerated on every code change when dev servers are running. No manual regeneration needed.

## MCP Tools Reference

When the apx MCP server is available, use these tools:

| Tool | Description |
|------|-------------|
| `start` | Start development server and return the URL |
| `stop` | Stop the development server |
| `restart` | Restart the development server (preserves port if possible) |
| `logs` | Fetch recent dev server logs |
| `check` | Check project code for errors (runs tsc and ty checks in parallel) |
| `refresh_openapi` | Regenerate OpenAPI schema and API client |
| `search_registry_components` | Search shadcn registry components using semantic search |
| `add_component` | Add a component to the project |
| `docs` | Search Databricks SDK documentation for code examples and API references |
| `databricks_apps_logs` | Fetch logs from deployed Databricks app using Databricks CLI |
| `get_route_info` | Get code example for using a specific API route |

## Lakebase Database Development

This project uses **Databricks Lakebase** for PostgreSQL database. Each developer/agent gets their own isolated branch.

### ‚ö†Ô∏è CRITICAL: Database Setup Order

**ALWAYS run `lakebase-branch.sh` FIRST before ANY database operations:**

```bash
./scripts/lakebase-branch.sh
```

**üö´ NEVER run `alembic` commands directly before running `lakebase-branch.sh`!**

Why this matters:
- The script creates YOUR isolated Lakebase branch
- Running migrations before the script would apply changes to the WRONG database (parent branch)
- Each worktree/agent gets its own database branch - migrations must target that specific branch

### Correct Order of Operations

1. **First:** Run `./scripts/lakebase-branch.sh` (creates branch, endpoint, credentials, runs migrations)
2. **Then:** Start development with `uv run apx dev start`
3. **Later:** If you add new migrations, run `uv run alembic upgrade head` (now targets YOUR branch)

The script handles everything including running migrations on your isolated branch.

### Branch Naming Convention

| Context | Lakebase Branch |
|---------|-----------------|
| Production (deployed app) | `production` (default branch) |
| Local dev on `main` | `{username}-main` (e.g., `alex-feng-main`) |
| Local dev on `feature-x` | `{username}-feature-x` |
| Agent worktree | `{username}-{parent-branch}-worktree-{worktree-dir}` |

Branch names are sanitized: slashes become hyphens, dots become hyphens, all lowercase.

### Databricks Postgres CLI Commands

**Requires Databricks CLI >= 0.286+**

```bash
# List branches in project
databricks postgres list-branches "projects/{project_id}"

# Create a new branch from source (with 6-hour TTL)
databricks postgres create-branch "projects/{project_id}" "{branch_name}" \
  --json '{"spec": {"source_branch_id": "{source_branch_id}", "ttl": {"seconds": 21600}}}'

# Create a branch with no expiry
databricks postgres create-branch "projects/{project_id}" "{branch_name}" \
  --json '{"spec": {"source_branch_id": "{source_branch_id}", "no_expiry": true}}'

# Create endpoint on branch (with autoscaling)
databricks postgres create-endpoint "projects/{project_id}/branches/{branch_id}" "dev" \
  --json '{"spec": {"endpoint_type": "ENDPOINT_TYPE_READ_WRITE", "autoscaling_limit_min_cu": 0.5, "autoscaling_limit_max_cu": 2.0}}'

# Generate database credentials (returns OAuth token)
databricks postgres generate-database-credential \
  "projects/{project_id}/branches/{branch_id}/endpoints/{endpoint_id}"
# Note: Use your email as username and the returned 'token' as password
```

### Database Migrations (Alembic)

**‚ö†Ô∏è Only run these commands AFTER `./scripts/lakebase-branch.sh` has been executed!**

```bash
# Apply all migrations (only after lakebase-branch.sh!)
uv run alembic upgrade head

# Create new migration after model changes
uv run alembic revision --autogenerate -m "description"

# Rollback last migration
uv run alembic downgrade -1

# Show current migration status
uv run alembic current
```

**Note:** The `lakebase-branch.sh` script automatically runs `alembic upgrade head` after setting up your branch. You only need to run migrations manually if you create new ones during development.

### Environment Configuration

- **Local dev:** `.env` file (auto-generated by `lakebase-branch.sh`)
- **Production:** Configured in `databricks.yml` env vars
- **NEVER** commit credentials to git (`.env` is gitignored)
